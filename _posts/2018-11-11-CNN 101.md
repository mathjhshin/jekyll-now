---
layout: post
title: CNN 101
---

Next you can update your site name, avatar and other options using the _config.yml file in the root of your repository (shown below).

![_config.yml]({{ site.baseurl }}/images/config.png)



# (1) Overview of Convolution Layer
## 1. Why Covolution?

왜 합성곱 신경망이 일반적인 신경망보다 성능이 좋을까?

합성곱 신경망이 성능을 개선하는 데 도움이 되는 세가지 중요한 개념이 있다.

- Sparse interaction

: 입력 단위가 m개, 출력 단위가 n개인 상황에서 Fully Connected Layer를 사용했다고 가정해보자.

이 경우, 행렬곱셈에 따른 실행 시간 복잡도는 O(m x n)이 된다.

그러나 만약, 출력의 연결 수를 최대 k로 제한한다면, Sparse interaction 방식에서 필요한  실행 시간 복잡도는 O(k x n)이 된다.

실제 Convolution Layer에서는 보통 k=3으로 여러개의 Channel을 두므로, O(C x k x n)의 Parameter가 필요하다.

이때, k =< C x k << m 으로 두게 되면, 모형의 메모리 사용량이 줄어들고 연산의 수도 줄어들게 되어 효율적이다.

또한, 이미지 처리를 예시로 들 때, 입력이미지가 수천~수백만 개의 픽셀로 이루어졌다고 하더라도, 윤곽선 등의 작고 의미 있는 특징들은 단 수십 또는 수백개의 픽셀로만 이루어진 Filter Kernel로 검출할 수 있다.

물론, 수백만 픽셀로 이루어진 Filter Kernel로도 검출할 수 있겠지만, 같은 것을 검출할 때 더 적은 수의 Parameter를 사용하여, 통계적으로 더 의미있는 결과를 얻기 쉬워진다.


- Parameter Sharing

: Fully Connected Layer의 경우, 한 층의 출력을 계산할 때 가중치 행렬의 각 성분은 딱 한 번씩만 사용된다.

이에 반해, Convolutioon Layer의 경우에는 kernel의 각 성분이 입력의 모든 곳에서, 모든 출력을 위해 사용된다.

이러한 Parameter Sharing으로 인해 몇 가지 성질이 생기게 되는데, 그 중 첫번째가 적은 메모리 요구량과 통계적 효율성이다.

실행시간은 여전히 O(k x n)이지만, 매개변수 공유 덕분에 저장해야 하는 Weight Parameter는 k x n이 아닌, k개가 된다.

Fully Connected Layer의 Weight Parameter의 개수인 m x n에 비해 엄청나게 적은 수이다.

두번째는 입력의 길이에 의존하지 않는 Weight Parameter의 개수로 인해, 가변 크기 입력을 처리할 수 있다는 점이다.

다만, 가변 크기의 입력들을 처리하기 위해 Covolution Layer를 사용하는 것은 같은 종류의 대상을 측정한 값들의 길이가 달라서(예를 들어 음향 자료의 녹음 시간) 입력의 크기가 다른 경우에만 유효하다.

입력 특징의 종류 자체가 달라지는 경우에는 유효하지 않다.

세번째는, 아래에 중요한 개념으로 들어갈, 이동(translation)에 대한 등변성(equivariance)이다.

- Equivariant Representation

: Convolution Layer의 경우, Parameter sharing으로 인해 이동(translation)에 대한 등변성(equivariance)이라는 성질이 생긴다.

이동에 대한 등변성이란, 이동 함수 g와 어떤 함수 f가 있을 때, $ f(g(x)) = g(f(x))$임을 만족하는 것이다.

따라서, 입력의 위치에 의존하지 않는 특징을 추출하게 된다.

## 2. How Covolution?

그렇다면, CNN은 일반적으로 어떻게 구성이 될까?

일반적으로 CNN의 한 층은 세가지 Stage로 구성된다.

Convolution stage, detector stage, pooling stage이다.

각각의 단계를 알아보자

- Convolution stage

: Convolution을 여러 Channel로 수행해서 선형 활성화 값을 구하는 단계이다.

- Detector stage

: 선형 활성화 값을 비선형 활성화 함수를 거쳐, 활성값을 검출하는 단계이다. 대표적으로 ReLU가 많이 사용된다.

- Pooling stage

: 특정 위치에서의 신경망의 출력을 근처 출력들의 요약통계량으로 대체하여, 그 층의 출력을 수정하는 단계이다. 대표적으로 MaxPooling이 있다. 어떤 Pooling을 사용하든, Pooling은 출력 표현이 입력의 작은 이동에 대해 근사적으로 변하지 않는데 도움을 준다. 즉, 국소적 불변성(Local invariance)의 성질을 준다.

![_config.yml]({{ site.baseurl }}/images/pooling_image.png)

## 3. Setting of Convolution

- padding: valid, same, temporal

- stride






# (2) The Variants of Convolution Layer

- Link1: [Convolution layer blog post](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)
- Link2: [Deformable Convolution blog post](https://medium.com/@phelixlau/notes-on-deformable-convolutional-networks-baaabbc11cf3)
- Link3: [CNN Architecture blog post](https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41)

## 1. Dilated Convolution



## 2. Deformable Convolution



## 3. Separable Convolution


## 4. Capsule Network



The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.
